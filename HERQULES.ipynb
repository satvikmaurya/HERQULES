{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the base libraries for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up libraries for data processing\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "## Required for NN\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import os\n",
    "from pstats import SortKey\n",
    "import time\n",
    "\n",
    "import matplotlib.font_manager\n",
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams['figure.figsize'] = [15, 9]\n",
    "# Set general font size\n",
    "#plt.rcParams['font.size'] = '16'\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "# plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "#plt.rcParams['font.serif'] = ['Helvetica']\n",
    "plt.rc('font', family='Dejavu Sans')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "#mpl.rcParams['figure.dpi'] = 200\n",
    "#from matplotlib import rc\n",
    "#rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "#rc('text', usetex=True)\n",
    "plt_colors = ['cornflowerblue', 'crimson', 'forestgreen', 'mediumorchid']\n",
    "\n",
    "fontsize = 32\n",
    "\n",
    "sns.set_theme(style='ticks')\n",
    "sns.set_context(\"poster\")\n",
    "plt.style.use('seaborn-deep')\n",
    "plt.style.use('seaborn-talk')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Datasets and scripts\n",
    "\n",
    "### Dataset \n",
    "\n",
    "* ADC\\_10kData\n",
    "    + description: the raw dataset\n",
    "    + location: data['Data']\n",
    "    + shape: (1600000, 1473, 2)\n",
    "        - 320000: 32 (#basis\\_states) * 10000 (#records per state)\n",
    "        - 1473: #samples in each trace where samples are collected in a 2ns rate\n",
    "        - 2: I and Q respectively\n",
    "        \n",
    "* all\\_traces.npy\n",
    "    + description: data extracted from 2019\\_ReadoutDataAfterADC\\_T0, did some rearrangement\n",
    "    + shape: (32, 10000, 1473, 2) -> (#basis\\_states, #records per state, #samples per record, I/Q)\n",
    "    \n",
    "* split\\_data/\\*\n",
    "    + description: split all\\_traces.npy into train/val/test\n",
    "    + split: train/val/test is 1950/1050/7000 for each basis state\n",
    "\n",
    "* matched\\_filter.py\n",
    "    + Matched filtering code used for the HERQULES for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_and_test_set(trace, y, num_qubits=5, \n",
    "                               NUM_TRAIN_VAL = 3000,\n",
    "                               NUM_TEST = 7000, NUM_VAL_RATIO = 0.35):\n",
    "    \n",
    "    NUM_VAL = int(NUM_TRAIN_VAL * NUM_VAL_RATIO)\n",
    "    NUM_TRAIN = NUM_TRAIN_VAL - NUM_VAL\n",
    "\n",
    "    ## Data for training, validation and testing is in train_set, val_set and test_set\n",
    "    ## Accordingly labels in y_train, y_val, y_test\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    y_train = []\n",
    "    y_val = []\n",
    "    y_test = []\n",
    "    for i in range(0, 2**num_qubits):\n",
    "        ind = np.where(y==i)[0]\n",
    "        train_set.append(trace[ind[:NUM_TRAIN], :, :])\n",
    "        test_set.append(trace[ind[NUM_TRAIN_VAL:], :, :])\n",
    "        val_set.append(trace[ind[NUM_TRAIN:NUM_TRAIN_VAL], :, :])\n",
    "        y_train.append(np.array([i for _ in range(NUM_TRAIN)]))\n",
    "        y_val.append(np.array([i for _ in range(NUM_VAL)]))\n",
    "        y_test.append(np.array([i for _ in range(NUM_TEST)]))\n",
    "    train_set = np.reshape(np.array(train_set), (2**num_qubits * NUM_TRAIN, trace.shape[1], trace.shape[2]))\n",
    "    val_set = np.reshape(np.array(val_set), (2**num_qubits * NUM_VAL, trace.shape[1], trace.shape[2]))\n",
    "    test_set = np.reshape(np.array(test_set), (2**num_qubits * NUM_TEST, trace.shape[1], trace.shape[2]))\n",
    "    y_train = np.reshape(np.array(y_train), (2**num_qubits * NUM_TRAIN))\n",
    "    y_val = np.reshape(np.array(y_val), (2**num_qubits * NUM_VAL))\n",
    "    y_test = np.reshape(np.array(y_test), (2**num_qubits * NUM_TEST))\n",
    "    return tuple((train_set, val_set, test_set)), tuple((np.array(y_train), np.array(y_val), np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demodulation process and pre_classifer_model data\n",
    "\n",
    "The demodulation class takes the raw ADC sample file and demodulates the readout signal into the constituent readout tones of the five qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "file_name = 'ADC_10kData'\n",
    "\n",
    "class demodulation():\n",
    "\tdef __init__(self, data_path = \"./\", file_name = 'ADC_10kData') -> None:\n",
    "\t\tself.data_path = data_path\n",
    "\t\tself.file_name = file_name\n",
    "        \n",
    "\tdef y_classes(self, states_bin, num_records=int(1e5), num_Q=1):\n",
    "\t\t\"\"\"Summary\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tstates_bin (TYPE): Description\n",
    "\t\t\tnum_records (TYPE, optional): Description\n",
    "\t\t\tnum_Q (int, optional): Description\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tTYPE: Description\n",
    "\t\t\"\"\"\n",
    "\t\t## assumption that same number of records per state configuration\n",
    "\t\t## assumption samples are sorted\n",
    "\t\tBinary \t\t= np.logspace(0.0, num_Q-1, num=num_Q, base=2.0)\n",
    "\t\tSta_help \t= np.zeros((2**num_Q,num_Q))\n",
    "\t\tfor i in range(len(states_bin)):\n",
    "\t\t\tSta_help[i,:] = np.remainder([ord(c) for c in states_bin[i]], 48)\n",
    "\n",
    "\t\treturn np.repeat(np.sum(np.multiply(Sta_help, Binary[::-1]), axis=1), num_records).astype(int)\n",
    "\n",
    "\tdef states_config(self, num_Q):\n",
    "\t\t\"\"\"Summary\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tnum_Q (TYPE): Description\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tTYPE: Description\n",
    "\t\t\"\"\"\n",
    "\t\t## different state configuration -> 2^(# of qubits)\n",
    "\t\tstates_bin = np.array(list(itertools.product([0,1], repeat=num_Q)))\n",
    "\t\tstates_bin_help = list(np.zeros(2**num_Q))\n",
    "\t\tfor i in range(2**num_Q):\n",
    "\t\t\tstates_bin_help[i] = str(states_bin[i,:])[1::2]\n",
    "\t\treturn np.array(states_bin_help)\n",
    "\n",
    "\tdef data_dem(self, states_bin, num_samples, sampling_rate, dFreq, num_Q=1, num_records=1e5, skip=0, DT_Bin=2e-9, NoF=0):\n",
    "\t\t\"\"\"Summary\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tstates_bin (TYPE): Description\n",
    "\t\t\tnum_samples (TYPE): Description\n",
    "\t\t\tsampling_rate (TYPE): Description\n",
    "\t\t\tdFreq (TYPE): Demodulation at frequency dFreq\n",
    "\t\t\tnum_Q (int, optional): Description\n",
    "\t\t\tnum_records (float, optional): Description\n",
    "\t\t\tskip (int, optional): Description\n",
    "\t\t\tDT_Bin (float, optional): Description\n",
    "\t\t\tNoF (int, optional): Description\n",
    "\t\t\"\"\"\n",
    "\t\t## digital demodulation at frequency freq_readout\n",
    "\t\tDDem = self.data_res_dem(sampling_rate, dFreq, num_samples, skip)\n",
    "\n",
    "\t\t## number of binned time steps\n",
    "\t\tT_D = int(sampling_rate*DT_Bin)\n",
    "\n",
    "\t\tif T_D > 1:\n",
    "\t\t\tnum_samples_bin = int(num_samples/T_D)\n",
    "\t\t\tprint('Bin length [ns]:', DT_Bin*1e9)\n",
    "\t\t\tprint('Number of binned data points:', T_D)\n",
    "\t\t\tprint('Raw number of binned samples:', num_samples_bin)\n",
    "\n",
    "\t\t\tDDem_help = DDem.shape\n",
    "\t\t\tDD = np.zeros([DDem_help[0],num_samples_bin,2])\n",
    "\t\t\tfor l in range(num_samples_bin):\n",
    "\t\t\t\tDD[:,l,:] = np.mean(DDem[:,l*T_D:(l+1)*T_D,:], axis = 1)\n",
    "\n",
    "\t\t\tprint('Binned raw processing data shape starting at t0:', DDem.shape)\n",
    "\t\telse: \n",
    "\t\t\tnum_samples_bin = num_samples\n",
    "\t\t\tDD = DDem\n",
    "\n",
    "\t\t## binned time axis\n",
    "\t\tt_bin = np.linspace(0,T_D*(num_samples_bin-1)/sampling_rate*1e6,num_samples_bin)\n",
    "\n",
    "\t\t## class labels (2**num_Q)\n",
    "\t\ty = self.y_classes(states_bin, num_records, num_Q)\n",
    "\n",
    "\t\t## creation of digitally demodulated data file\n",
    "\t\tif bool(NoF):\n",
    "\t\t\thf = h5py.File('%s/DD_10k_f%i_v%i' % (data_path, NoF, 1), 'w')\n",
    "\t\telse:\n",
    "\t\t\thf = h5py.File('%s/DD_10k%iQ_v%i' % (data_path, num_Q, 1), 'w')\n",
    "\t\thf.create_dataset('DD', \tdata=DD)\n",
    "\t\thf.create_dataset('y', \t\tdata=y)\n",
    "\t\thf.create_dataset('t_bin', \tdata=t_bin)\n",
    "\t\thf.close()\n",
    "\n",
    "\n",
    "\tdef data_IQ_dem(self, DataI, DataQ, dt, dFreq, dlen, skip=0):\n",
    "\t\t\"\"\"Summary\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tDataI (TYPE): Description\n",
    "\t\t\tDataQ (TYPE): Description\n",
    "\t\t\tdt (TYPE): Description\n",
    "\t\t\tdFreq (TYPE): Description\n",
    "\t\t\tdlen (TYPE): Description\n",
    "\t\t\tskip (int, optional): Description\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tTYPE: Description\n",
    "\t\t\"\"\"\n",
    "\t\t## digital demodulation of analog demodulated I and Q at frequency dFreq\n",
    "\t\thelp_d \t= DataI.shape\n",
    "\t\tdlen \t= int(min(dlen, help_d[1]))\n",
    "\t\t\n",
    "\t\t# normalization to compensate for IQ mixer offset\n",
    "\t\tDataI \t= DataI-np.mean(DataI)\n",
    "\t\tDataQ \t= DataQ-np.mean(DataQ)\n",
    "\t\t\n",
    "\t\t# normalization for amplitude error IQ mixer\n",
    "\t\tCorrFa \t= np.std(DataI)/np.std(DataQ)\n",
    "\t\tDataQ \t= DataQ*CorrFa\n",
    "\t\t\n",
    "\t\t# calculate cos/sin vectors, allow segmenting\n",
    "\t\tvTime \t= dt*(skip+np.arange(dlen-skip, dtype = float))\n",
    "\t\tvCos \t= np.cos(2*np.pi*vTime*dFreq)\n",
    "\t\tvSin \t= np.sin(2*np.pi*vTime*dFreq)\n",
    "\t\t\n",
    "\t\tdel CorrFa, vTime, dFreq, help_d\n",
    "\n",
    "\t\t# calculate demodulated I/Q from I/Q outputs of mixer  \n",
    "\t\tdI \t\t= 2*(vCos*DataI[:,skip:dlen]+vSin*DataQ[:,skip:dlen])\n",
    "\t\tdQ \t\t= 2*(vCos*DataQ[:,skip:dlen]-vSin*DataI[:,skip:dlen])\n",
    "\t\treturn (dI, dQ)\n",
    "\n",
    "\tdef data_res_dem(self, sampling_rate, dFreq, dlen, skip=0):\n",
    "\t\t\"\"\"Summary\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tsampling_rate (TYPE): Description\n",
    "\t\t\tdFreq (TYPE): Description\n",
    "\t\t\tdlen (TYPE): Description\n",
    "\t\t\tskip (int, optional): Description\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tTYPE: Description\n",
    "\t\t\"\"\"\n",
    "\t\tprint(self.data_path, self.file_name)\n",
    "\t\t## open analog demodulated data file\n",
    "\t\thf = h5py.File('%s/%s' % (self.data_path, self.file_name), 'r')\n",
    "\t\tData_to_Demod \t= hf.get('Data')\n",
    "\t\t\n",
    "\t\t## create digital demodulated data file at frequency freq_readout\n",
    "\t\t(DDemI, DDemQ) = self.data_IQ_dem(DataI=Data_to_Demod[:,:,0], DataQ=Data_to_Demod[:,:,1], dt=1./sampling_rate, dFreq=dFreq, dlen=dlen, skip=0)\n",
    "\t\thf.close()\n",
    "\t\treturn np.stack((DDemI,DDemQ), axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demodulating the readout signal\n",
    "\n",
    "The resonator frequencies for the 5 constituent qubits is in `freq_readout`. `DT_bin` defines the binning of readout samples that can be used to define the number of samples that are averaged together to represent one sample of the demodulated signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demod = demodulation()\n",
    "num_Q \t\t\t= 5 \t\t# number of qubits\n",
    "states_bin \t\t= demod.states_config(num_Q)\n",
    "sampling_rate \t= 500*1e6 #Samples/sec\n",
    "num_samples \t= int(2e-6*sampling_rate)\n",
    "freq_readout \t= -np.array([-64.729*1e6,-25.366*1e6,24.79*1e6,70.269*1e6,127.282*1e6]) #Hz\n",
    "num_records \t= int(1e4) #number of repeated acquisitions\t\n",
    "DT_Bin\t\t\t= 2e-9 # or higher\n",
    "\n",
    "if freq_readout.size>1:\n",
    "\t\tfor i in range(freq_readout.size):\n",
    "\t\t\tdemod.data_dem(states_bin, num_samples, sampling_rate, freq_readout[i], num_Q, num_records, 0, DT_Bin, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying relaxation traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x0, y0, x1, y1):\n",
    "    return np.sqrt((x0 - x1)**2 + (y0 - y1)**2)\n",
    "\n",
    "def get_data(qubit):\n",
    "    file = './DD_10k_f%i_v1'%(qubit)\n",
    "    data = h5py.File(file, 'r')\n",
    "    traces = np.array(data['DD'])\n",
    "    y = np.array(data['y'])\n",
    "    t = np.array(data['t_bin'])\n",
    "    return traces, y, t\n",
    "\n",
    "getbinary = lambda x, n: format(x, 'b').zfill(n)\n",
    "\n",
    "def get_traces(num_qubits=5, plot=False, rscale=1, data_type=0):\n",
    "    qubit_traces = {}\n",
    "    indices_0 = np.arange(10000)\n",
    "    one_indices = {2**(qubit - 1):None for qubit in range(1, num_qubits+1)}\n",
    "    for qubit in range(1, num_qubits+1):\n",
    "        dem, y, t = get_data(qubit)\n",
    "        #indices_0 = np.arange(np.array(dem).shape[0])\n",
    "        traces, ys = get_train_val_and_test_set(dem, y)\n",
    "        dem = traces[data_type]\n",
    "        y = ys[data_type]\n",
    "        del traces\n",
    "        traces = dem\n",
    "        # Compute the centroid of the clouds corresponding to 0 and 1\n",
    "        zero = 0\n",
    "        one = 2**(qubit - 1)\n",
    "        i0 = np.mean(traces[np.where(y==zero)[0], :, 0], axis=1)\n",
    "        q0 = np.mean(traces[np.where(y==zero)[0], :, 1], axis=1)\n",
    "        i1 = np.mean(traces[np.where(y==one)[0], :, 0], axis=1)\n",
    "        q1 = np.mean(traces[np.where(y==one)[0], :, 1], axis=1)\n",
    "        i = np.mean(traces[:, :, 0], axis=1)\n",
    "        q = np.mean(traces[:, :, 1], axis=1)\n",
    "        x0 = np.mean(i0)\n",
    "        y0 = np.mean(q0)\n",
    "        x1 = np.mean(i1)\n",
    "        y1 = np.mean(q1)\n",
    "        midx = (x0 + x1) / 2\n",
    "        midy = (y0 + y1) / 2\n",
    "        radius = rscale * distance(x0, y0, x1, y1) / 2 # Radius from the centroid where a trace is considered correct\n",
    "        traces_0 = traces[np.where(y==zero)[0], :, :]\n",
    "        traces_1 = traces[np.where(y==one)[0], :, :]\n",
    "        traces_0 = traces_0[np.where(distance(i0, q0, x0, y0) < radius)[0], :, :]\n",
    "        traces_1 = traces_1[np.where(distance(i1, q1, x1, y1) < radius)[0], :, :]\n",
    "        indices_0 = np.intersect1d(indices_0, np.where((y==zero) & (distance(i, q, x0, y0) < radius))[0])\n",
    "        indices_1 = np.where((y==one) & (distance(i, q, x1, y1) < radius))[0]\n",
    "        \n",
    "        # Find the new centroids with the purified traces\n",
    "        x0_filtered = np.mean(traces_0[:, :, 0], axis=1)\n",
    "        y0_filtered = np.mean(traces_0[:, :, 1], axis=1)\n",
    "        x1_filtered = np.mean(traces_1[:, :, 0], axis=1)\n",
    "        y1_filtered = np.mean(traces_1[:, :, 1], axis=1)\n",
    "        one_indices[one] = indices_1\n",
    "        \n",
    "        if plot:\n",
    "            print('New # traces:%i; old: %i'%(traces_0.shape[0] + traces_1.shape[0], i0.shape[0] + i1.shape[0]))\n",
    "        \n",
    "        new_i0 = np.mean(traces_0[:, :, 0], axis=1)\n",
    "        new_q0 = np.mean(traces_0[:, :, 1], axis=1)\n",
    "        new_i1 = np.mean(traces_1[:, :, 0], axis=1)\n",
    "        new_q1 = np.mean(traces_1[:, :, 1], axis=1)\n",
    "\n",
    "        # Found correct traces for ground and excited states\n",
    "        # Now to find traces corresponding to 1->0 relaxations and 0->1 excitations\n",
    "        # Also need to distinguish between initialization errors and relaxations/excitations\n",
    "        # Find ground truth (0/1) traces\n",
    "\n",
    "        ground_0_trace = np.mean(traces_0, axis=0)\n",
    "        ground_1_trace = np.mean(traces_1, axis=0)\n",
    "\n",
    "        # Relaxation traces vs. |2> state traces:\n",
    "        # |2> traces are generally in a different direction than relaxation traces;\n",
    "        # Vector of relaxation trace -> ground state trace\n",
    "        # Use centroid of ground state cluster with the incorrect |1> trace to determine\n",
    "        # vector direction. Then if direction is similar to the vector joining the two\n",
    "        # Centroids (0, 1), then it is most likely a relaxation trace. \n",
    "\n",
    "        incorrect_traces_0 = traces[np.where(y==zero)[0], :, :]\n",
    "        incorrect_traces_1 = traces[np.where(y==one)[0], :, :]\n",
    "        incorrect_traces_0 = incorrect_traces_0[np.where(distance(i0, q0, x0, y0) >= rscale * radius)[0], :, :]\n",
    "        incorrect_traces_1 = incorrect_traces_1[np.where(distance(i1, q1, x1, y1) >= rscale * radius)[0], :, :]\n",
    "\n",
    "        # Find traces showing relaxation by evaluation if the mean of the trace lies in\n",
    "        # the circle corresponding to the ground state\n",
    "        incorrect_i1 = np.mean(incorrect_traces_1[:, :, 0], axis=1)\n",
    "        incorrect_q1 = np.mean(incorrect_traces_1[:, :, 1], axis=1)\n",
    "        relax_traces_1 = incorrect_traces_1[np.where(distance(incorrect_i1, incorrect_q1, x0, y0) <= rscale * radius)[0], :, :]\n",
    "        relax_trace = np.mean(relax_traces_1, axis=0)\n",
    "        ket_2_traces = incorrect_traces_1[np.where(distance(incorrect_i1, incorrect_q1, x0, y0) > rscale * radius)[0], :, :]\n",
    "        ket_2_trace = np.mean(ket_2_traces, axis=0)\n",
    "        excitation_traces_0 = incorrect_traces_0\n",
    "        excitation_trace = np.mean(excitation_traces_0, axis=0)\n",
    "\n",
    "        data = {}\n",
    "        data['gnd_0'] = ground_0_trace\n",
    "        data['gnd_1'] = ground_1_trace\n",
    "        data['relax'] = relax_trace\n",
    "        data['ket2'] = ket_2_traces\n",
    "        data['excite'] = excitation_trace\n",
    "        data['mean_0'] = tuple((x0, y0))\n",
    "        data['mean_1'] = tuple((x1, y1))\n",
    "        data['mean_0_filtered'] = tuple((np.mean(x0_filtered), np.mean(y0_filtered)))\n",
    "        data['mean_1_filtered'] = tuple((np.mean(x1_filtered), np.mean(y1_filtered)))\n",
    "        data['traces_relax'] = relax_traces_1\n",
    "        data['traces_0'] = traces_0\n",
    "        data['traces_1'] = traces_1\n",
    "        data['traces_excite'] = incorrect_traces_0\n",
    "        qubit_traces[qubit] = data\n",
    "\n",
    "        if plot:\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(27, 9), constrained_layout=True, sharex='all', sharey='all')\n",
    "            ax = axs[0]\n",
    "            ax.scatter(i0, q0, label='0', alpha=1)\n",
    "            ax.scatter(i1, q1, label='1', alpha=0.1)\n",
    "            ax.scatter(x0, y0, color='black')\n",
    "            ax.scatter(x1, y1, color='black')\n",
    "            ax.scatter(np.mean(x0_filtered), np.mean(y0_filtered), color='crimson')\n",
    "            ax.scatter(np.mean(x1_filtered), np.mean(y1_filtered), color='crimson')\n",
    "            ax.plot([x0, x1], [y0, y1], color='black', linestyle='--')\n",
    "            ax.scatter(midx, midy, color='crimson')\n",
    "            ax.legend()\n",
    "            ax.set_title('Original')\n",
    "            ax = axs[1]\n",
    "            ax.scatter(new_i0, new_q0, label='0')\n",
    "            ax.scatter(new_i1, new_q1, label='1', alpha=0.1)\n",
    "            ax.legend()\n",
    "            ax.set_title('After filtering')\n",
    "            plt.suptitle('Qubit %i'%(qubit))\n",
    "            plt.show()\n",
    "            print(\"Correct 0 traces: %i\"%(traces_0.shape[0]))\n",
    "            print(\"Correct 1 traces: %i\"%(traces_1.shape[0]))\n",
    "            print(\"Qubit relaxation traces: %i\"%(relax_traces_1.shape[0]))\n",
    "            print(\"|2> traces: %i\"%(ket_2_traces.shape[0]))\n",
    "            print(\"Excitation traces: %i\"%(excitation_traces_0.shape[0]))\n",
    "\n",
    "    indices_y = np.zeros(indices_0.shape)\n",
    "    indices_1 = np.array([])\n",
    "    for key in one_indices.keys():\n",
    "        indices_y = np.hstack((indices_y, key * np.ones(one_indices[key].shape)))\n",
    "        indices_1 = np.hstack((indices_1, one_indices[key]))\n",
    "    filtered_indices = tuple((indices_0, one_indices, indices_1, indices_y))\n",
    "    return qubit_traces, filtered_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_info, _ = get_traces(plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_traces(traces_info, qubit):\n",
    "    qubit_traces_info = traces_info[qubit]\n",
    "    ground_0_trace = qubit_traces_info['gnd_0']\n",
    "    ground_1_trace = qubit_traces_info['gnd_1']\n",
    "    relax_trace = qubit_traces_info['relax']\n",
    "\n",
    "    plt_ground_0_trace = ground_0_trace.reshape(-1, 25, ground_0_trace.shape[1]).mean(axis = 1)\n",
    "    plt_ground_1_trace = ground_1_trace.reshape(-1, 25, ground_1_trace.shape[1]).mean(axis = 1)\n",
    "    plt_relax_trace = relax_trace.reshape(-1, 25, relax_trace.shape[1]).mean(axis = 1)\n",
    "\n",
    "\n",
    "    plt.plot(plt_ground_0_trace[:, 0], plt_ground_0_trace[:, 1], label='0')\n",
    "    plt.plot(plt_ground_1_trace[:, 0], plt_ground_1_trace[:, 1], label='1')\n",
    "    plt.plot(plt_relax_trace[:, 0], plt_relax_trace[:, 1], label='1->0 relaxation')\n",
    "    plt.xlabel('I (In-Phase)')\n",
    "    plt.ylabel('Q (Quadrature)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_data_traces(traces_info, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mf(traces_0, traces_1):\n",
    "    traces_0 = traces_0.reshape(-1, traces_0.shape[1] * traces_0.shape[2])\n",
    "    traces_1 = traces_1.reshape(-1, traces_1.shape[1] * traces_1.shape[2])\n",
    "    # traces_0 generally more than traces_1 traces. \n",
    "    # Do random sampling from the trace_0 set\n",
    "    mf = None\n",
    "    if traces_1.shape[0] > traces_0.shape[0]:\n",
    "        indices = np.random.choice(traces_1.shape[0], traces_0.shape[0], replace=False)\n",
    "        mf = np.mean(traces_0 - traces_1[indices], axis=0) / np.var(traces_0 - traces_1[indices], axis=0)\n",
    "    else:\n",
    "        indices = np.random.choice(traces_0.shape[0], traces_1.shape[0], replace=False)\n",
    "        mf = np.mean(traces_0[indices] - traces_1, axis=0) / np.var(traces_0[indices] - traces_1, axis=0)\n",
    "    \n",
    "    filtered = np.sum(np.multiply(mf, traces_0), axis=1)\n",
    "    filtered = np.sort(filtered)\n",
    "    ind = int(0.995 * filtered.shape[0])\n",
    "    threshold = filtered[ind]\n",
    "\n",
    "    return mf, threshold\n",
    "\n",
    "def plot_matched_filter(zero_traces, one_traces, title='Matched Filter'):\n",
    "    mf, thresh = get_mf(zero_traces, one_traces) \n",
    "    fig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(6, 4))\n",
    "    ax.plot(mf, label='MF Envelope')\n",
    "    ax.legend(fontsize=20)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "\n",
    "qubit =3\n",
    "qubit_traces_info = traces_info[qubit]\n",
    "traces_0 = qubit_traces_info['traces_0']\n",
    "traces_1 = qubit_traces_info['traces_1']\n",
    "traces_relax = qubit_traces_info['traces_relax']\n",
    "\n",
    "plot_matched_filter(traces_0, traces_1, title='Plotting matched filter for zero traces vs one traces')\n",
    "plot_matched_filter(traces_relax, traces_0, title='Plotting matched filter for zero traces vs relaxation traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some helper classes for defining preclassifiers for detecting relaxations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preclassifier(demodulation):\n",
    "    # Can pass train-test ratio and other parameters through constructor later.\n",
    "    def __init__(self, radius_scale=1) -> None:\n",
    "        self.filtered_indices = None\n",
    "        self.rscale = radius_scale\n",
    "        self.trace_classes = None\n",
    "\n",
    "    def distance(self, x0, y0, x1, y1):\n",
    "        return np.sqrt((x0 - x1)**2 + (y0 - y1)**2)\n",
    "    \n",
    "    def fit(self):\n",
    "        qubit_traces, indices_filtered = get_traces()\n",
    "        self.filtered_indices = indices_filtered\n",
    "        self.trace_classes = qubit_traces\n",
    "        return\n",
    "\n",
    "    def predict(self, data, num_qubits=5):\n",
    "        data = np.array(data)\n",
    "        temp = None\n",
    "        if len(data.shape) == 3:\n",
    "            indices_0 = self.filtered_indices[0]\n",
    "            indices_1 = self.filtered_indices[2]\n",
    "            y = self.filtered_indices[3]\n",
    "            one_indices = self.filtered_indices[1]\n",
    "            temp = data[indices_0.astype(int), :, :]\n",
    "            temp = np.vstack((temp, data[indices_1.astype(int), :, :]))\n",
    "            return temp, y\n",
    "        else:\n",
    "            print('Please pass a dataset of the correct shape.')\n",
    "            return -1\n",
    "        return\n",
    "\n",
    "    def get_traces(self):\n",
    "        return self.trace_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relaxation_mf_classifier(preclassifier):\n",
    "    def __init__(self) -> None:\n",
    "        self.envelopes = []\n",
    "        self.thresholds = []\n",
    "        pass\n",
    "\n",
    "    def fit(self, trace_classes, num_qubits=5, boxcars=None):\n",
    "        for qubit in range(1, 1 + num_qubits):\n",
    "            # Distinguish between |0> and |1> -> |0> traces.\n",
    "            relaxation_traces = trace_classes[qubit]['traces_relax']\n",
    "            zero_traces = trace_classes[qubit]['traces_0']\n",
    "            relaxation_traces = relaxation_traces.reshape(-1, relaxation_traces.shape[1] * relaxation_traces.shape[2])\n",
    "            zero_traces = zero_traces.reshape(-1, zero_traces.shape[1] * zero_traces.shape[2])\n",
    "            # zero traces generally more than relaxation traces. \n",
    "            # Do random sampling from the zero trace set\n",
    "            mf = None\n",
    "            if zero_traces.shape[0] > relaxation_traces.shape[0]:\n",
    "                indices = np.random.choice(zero_traces.shape[0], relaxation_traces.shape[0], replace=False)\n",
    "                mf = np.mean(relaxation_traces - zero_traces[indices], axis=0) / np.var(relaxation_traces - zero_traces[indices], axis=0)\n",
    "            else:\n",
    "                indices = np.random.choice(relaxation_traces.shape[0], zero_traces.shape[0], replace=False)\n",
    "                mf = np.mean(relaxation_traces[indices] - zero_traces, axis=0) / np.var(relaxation_traces[indices] - zero_traces, axis=0)\n",
    "            if boxcars is not None:\n",
    "                boxcar = np.heaviside((len(mf) - boxcars[qubit - 1] / 50) - np.arange(len(mf)), 1)\n",
    "                mf = mf * boxcar\n",
    "            zero_filter = np.sum(np.multiply(mf, zero_traces), axis=1)\n",
    "            zero_filter = np.sort(zero_filter)\n",
    "            ind = int(0.995 * zero_filter.shape[0])\n",
    "            threshold = zero_filter[ind]\n",
    "            self.envelopes.append(mf)\n",
    "            self.thresholds.append(threshold)\n",
    "        return\n",
    "\n",
    "    def predict(self, num_qubits=5, data_type=0):\n",
    "        # Use demodulated data dumps\n",
    "        result = []\n",
    "        for qubit in range(1, num_qubits + 1):\n",
    "            dem, y, t = get_data(qubit)\n",
    "            traces, ys = get_train_val_and_test_set(np.array(dem), np.array(y))\n",
    "            dem = traces[data_type]\n",
    "            y = ys[data_type]\n",
    "            del traces\n",
    "            traces = dem\n",
    "            states = np.unique(y)\n",
    "            traces = traces.reshape(traces.shape[0], -1)\n",
    "            mf_results = np.empty((states.shape[0], int(traces.shape[0] / states.shape[0])))\n",
    "            for state in states:\n",
    "                mf_results[state] = np.array([np.sum(traces[y==state] * self.envelopes[qubit - 1], axis=1)])\n",
    "            result.append(mf_results)\n",
    "        result = np.array(result).transpose([1, 2, 0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a large NN classifier on the dataset\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "1. **Input Layer**\n",
    "   - **1000 Neurons**\n",
    "\n",
    "2. **Hidden Layer 1**\n",
    "   - **500 Neurons**\n",
    "\n",
    "3. **Hidden Layer 2**\n",
    "   - **250 Neurons**\n",
    "\n",
    "4. **Output Layer**\n",
    "   - **32 Neurons**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './'\n",
    "\n",
    "class ADCDataset(T.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        all_data = np.load(data_file)\n",
    "        num_samples_per_state = all_data.shape[1]\n",
    "        num_basis_state = all_data.shape[0]\n",
    "\n",
    "        all_labels = []\n",
    "        for i in range(num_basis_state):\n",
    "            all_labels.append(np.array([i for _ in range(num_samples_per_state)]))\n",
    "\n",
    "        all_data = all_data.reshape((num_basis_state * num_samples_per_state, -1))\n",
    "        all_data = all_data[:, :1000]  # only use the first 1000 samples (500 I's and 500 Q's)\n",
    "        all_labels = np.array(all_labels).reshape((-1))\n",
    "\n",
    "        self.x_data = T.tensor(all_data, dtype=T.float)\n",
    "        self.y_data = T.tensor(all_labels, dtype=T.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if T.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        preds = self.x_data[idx]\n",
    "        lbl = self.y_data[idx]\n",
    "        sample = {'predictors': preds, 'target': lbl}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def adjust_learning_rate(initial_lr, optimizer, epoch, lr_schedule=[30, 60, 90]):\n",
    "    lr = initial_lr\n",
    "    if epoch >= lr_schedule[0]:\n",
    "        lr *= 0.1\n",
    "\n",
    "    if epoch >= lr_schedule[1]:\n",
    "        lr *= 0.1\n",
    "\n",
    "    if epoch >= lr_schedule[2]:\n",
    "        lr *= 0.1\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def inference(model, dl):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    s = T.nn.Softmax(dim=-1)\n",
    "    with T.no_grad():\n",
    "        for (batch_idx, batch) in enumerate(dl):\n",
    "            X = batch['predictors']\n",
    "            Y = batch['target']\n",
    "            oupt = model(X)\n",
    "            oupt = s(oupt)\n",
    "\n",
    "            all_scores.extend(oupt.cpu().numpy())\n",
    "            all_labels.extend(Y.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    return np.array(all_scores), np.array(all_labels)\n",
    "\n",
    "\n",
    "def accuracy(model, dl):\n",
    "    all_preds, all_labels = inference(model, dl)\n",
    "    # logger.debug(all_preds.shape)\n",
    "\n",
    "    pred_indices = np.argmax(all_preds, axis=-1)\n",
    "    cumulative_acc = np.sum(pred_indices == all_labels) / len(all_labels)\n",
    "\n",
    "    acc_per_qubit = []\n",
    "    for _ in range(5):\n",
    "        pred_qubit = pred_indices % 2\n",
    "        label_qubit = all_labels % 2\n",
    "        acc_per_qubit.append(np.sum(pred_qubit == label_qubit) / len(label_qubit))\n",
    "        pred_indices = pred_indices >> 1\n",
    "        all_labels = all_labels >> 1\n",
    "\n",
    "    return cumulative_acc, acc_per_qubit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_all(NUM_TRAIN_VAL=3000, NUM_TEST = 7000, NUM_VAL_RATIO = 0.35):\n",
    "    NUM_VAL = int(NUM_TRAIN_VAL * NUM_VAL_RATIO)\n",
    "    NUM_TRAIN = NUM_TRAIN_VAL - NUM_VAL\n",
    "    TOTAL_TRACES = NUM_TRAIN_VAL + NUM_TEST\n",
    "    print('length of training set: {}'.format(NUM_TRAIN))\n",
    "    print('length of val set: {}'.format(NUM_VAL))\n",
    "    print('length of test set: {}'.format(NUM_TEST))\n",
    "    \n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "\n",
    "    data = np.load('all_traces_10k.npy')\n",
    "    print(data.shape)\n",
    "    \n",
    "    for basis_state_data in data:\n",
    "        random.shuffle(basis_state_data)\n",
    "        train_set.append(basis_state_data[:NUM_TRAIN])\n",
    "        val_set.append(basis_state_data[NUM_TRAIN:NUM_TRAIN_VAL])\n",
    "        test_set.append(basis_state_data[NUM_TRAIN_VAL:])\n",
    "\n",
    "    train_set = np.array(train_set)\n",
    "    print('Train set: ', train_set.shape)\n",
    "    val_set = np.array(val_set)\n",
    "    print('Val set: ', val_set.shape)\n",
    "    test_set = np.array(test_set)\n",
    "    print('Test set: ', test_set.shape)\n",
    "\n",
    "\n",
    "    os.makedirs('split_data', exist_ok=True)\n",
    "    np.save('split_data/train.npy', train_set)\n",
    "    np.save('split_data/val.npy', val_set)\n",
    "    np.save('split_data/test.npy', test_set)\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def data_load(num_Q=5, rep_seq=False):\n",
    "\n",
    "    ## structure: [QB5, QB4, QB3, QB2, QB1]\n",
    "    data = h5py.File('ADC_10kData', 'r')\n",
    "    num_records \t= int(1e4) #number of repeated acquisitions\n",
    "\n",
    "    print('loading Ch1')\n",
    "    Ch1 \t\t= data['Data'][:, :, 0]  # shape (1600000, 1473) = (32 * num_records, num_samples)\n",
    "    print('loading Ch2')\n",
    "    Ch2 \t\t= data['Data'][:, :, 1]  # shape (1600000, 1473) = (32 * num_records, num_samples)\n",
    "\n",
    "    ## statistics\n",
    "    num_samples_raw = Ch1.shape[1] #number of samples per acquisition\n",
    "    print('num_samples_raw: ', num_samples_raw)\n",
    "\n",
    "    ## data preparation\n",
    "    DataRaw = np.zeros((2**num_Q, num_records, num_samples_raw, 2))\n",
    "    DataRaw[:, :, :, 0] = np.reshape(Ch1, (2 ** num_Q, num_records, num_samples_raw))\n",
    "    DataRaw[:, :, :, 1] = np.reshape(Ch2, (2 ** num_Q, num_records, num_samples_raw))\n",
    "    print('DataRaw shape: ', DataRaw.shape)\n",
    "    \n",
    "    np.save('all_traces_10k.npy', DataRaw)\n",
    "    data.close()\n",
    "    del Ch1, Ch2\n",
    "    return\n",
    "\n",
    "data_load(num_Q=5, rep_seq=False)\n",
    "train_set, val_set, test_set = load_data_all(NUM_TRAIN_VAL=3000, NUM_TEST=7000, NUM_VAL_RATIO=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_baseline(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_baseline, self).__init__()\n",
    "        self.hid1 = T.nn.Linear(1000, 500)\n",
    "        self.hid2 = T.nn.Linear(500, 250)\n",
    "        self.oupt = T.nn.Linear(250, 32)\n",
    "\n",
    "        T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        T.nn.init.zeros_(self.hid1.bias)\n",
    "        T.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        T.nn.init.zeros_(self.hid2.bias)\n",
    "        T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        T.nn.init.zeros_(self.oupt.bias)\n",
    "        self.relu = T.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.hid1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.hid2(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.oupt(z)\n",
    "        return z\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def train_baseline():\n",
    "    # 0. get started\n",
    "    T.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # 1. create Dataset and DataLoader objects\n",
    "\n",
    "    train_ds = ADCDataset(os.path.join(root_dir, 'split_data/train.npy'))\n",
    "    val_ds = ADCDataset(os.path.join(root_dir, 'split_data/val.npy'))\n",
    "    test_ds = ADCDataset(os.path.join(root_dir, 'split_data/test.npy'))\n",
    "\n",
    "    bat_size = 512\n",
    "    train_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True)\n",
    "    val_ldr = T.utils.data.DataLoader(val_ds, batch_size=bat_size, shuffle=False)\n",
    "    test_ldr = T.utils.data.DataLoader(test_ds, batch_size=bat_size, shuffle=False)\n",
    "\n",
    "    # 2. create neural network\n",
    "    # Creating 1000-500-250-32 binary NN classifier \")\n",
    "    net = Net_baseline()  # .to(device)\n",
    "\n",
    "    # 3. train network\n",
    "    net = net.train()  # set training mode\n",
    "    lrn_rate = 0.0001\n",
    "    loss_obj = T.nn.CrossEntropyLoss()  # cross entropy\n",
    "    optimizer = T.optim.Adam(net.parameters(), lr=lrn_rate)\n",
    "    max_epochs = 100\n",
    "    ep_log_interval = 1\n",
    "\n",
    "\n",
    "    best_acc = -1\n",
    "\n",
    "    path = os.path.join(\".\", \"checkpoints/1000_points\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for epoch in range(0, max_epochs):\n",
    "        epoch_loss = 0.0  # for one full epoch\n",
    "\n",
    "        lr = adjust_learning_rate(lrn_rate, optimizer, epoch)\n",
    "\n",
    "        for (batch_idx, batch) in enumerate(train_ldr):\n",
    "            X = batch['predictors']\n",
    "            Y = batch['target']\n",
    "            oupt = net(X)\n",
    "\n",
    "            loss_val = loss_obj(oupt, Y)\n",
    "            epoch_loss += loss_val.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % ep_log_interval == 0:\n",
    "            print(\"epoch = %4d   loss = %0.4f  lr = %0.4f\" % \\\n",
    "                  (epoch, epoch_loss, lr))\n",
    "\n",
    "        acc_train, acc_train_per_qubit = accuracy(net, train_ldr)\n",
    "        acc_val, acc_val_per_qubit = accuracy(net, val_ldr)\n",
    "\n",
    "        if acc_val >= best_acc:\n",
    "            best_acc = acc_val\n",
    "            T.save(net.state_dict(), os.path.join(path, 'best_epoch.pth'))\n",
    "\n",
    "    print(\"Finished  training\")\n",
    "    print(\"Best acc on val dataset: {}\".format(best_acc))\n",
    "\n",
    "    net.load_state_dict(T.load(os.path.join(path, 'best_epoch.pth')))\n",
    "    acc_test, acc_test_per_qubit = accuracy(net, test_ldr)\n",
    "    print(\"Acc on test dataset: {}\".format(acc_test))\n",
    "    print(\"Acc per qubit: {}\".format(acc_test_per_qubit))\n",
    "\n",
    "train_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running HERQULES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matched_filter import search_matched_filter_for_all_qubits, matched_filter_preprocess, search_matched_filter_for_all_qubits_preclass, matched_filter_preprocess_demux, search_matched_filter_for_all_qubits_demux\n",
    "\n",
    "\n",
    "hf = h5py.File('./ADC_10kData', 'r')\n",
    "train_semi_sup_data = hf.get('Data')\n",
    "\n",
    "os.makedirs('accuracy', exist_ok=True)\n",
    "os.makedirs('stats', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "#root_dir = '/nobackup/readout_data/new'\n",
    "root_dir = '.'\n",
    "# root_dir ='.'\n",
    "\n",
    "def mf_demux_data_prep():\n",
    "    # Prepare data\n",
    "    data_train = []\n",
    "    data_val = []\n",
    "    data_test = []\n",
    "    for qubit in range(1, 6):\n",
    "        traces, y, t = get_data(qubit)\n",
    "        traces, y = get_train_val_and_test_set(np.array(traces), np.array(y))\n",
    "        data = traces[0].reshape(32, int(traces[0].shape[0] / 32), traces[0].shape[1], traces[0].shape[2])\n",
    "        data_train.append(data)\n",
    "        data = traces[1].reshape(32, int(traces[1].shape[0] / 32), traces[1].shape[1], traces[1].shape[2])\n",
    "        data_val.append(data)\n",
    "        data = traces[2].reshape(32, int(traces[2].shape[0] / 32), traces[2].shape[1], traces[2].shape[2])\n",
    "        data_test.append(data)\n",
    "    return data_train, data_val, data_test\n",
    "\n",
    "class MFOutputDataset(T.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, all_data):\n",
    "        num_samples_per_state = all_data.shape[1]\n",
    "        num_basis_state = all_data.shape[0]\n",
    "\n",
    "        all_labels = []\n",
    "        for i in range(num_basis_state):\n",
    "            all_labels.append(np.array([i for _ in range(num_samples_per_state)]))\n",
    "\n",
    "        all_data = all_data.reshape((num_basis_state * num_samples_per_state, -1))\n",
    "        all_labels = np.array(all_labels).reshape((-1))\n",
    "\n",
    "        self.x_data = T.tensor(all_data, dtype=T.float)\n",
    "        self.y_data = T.tensor(all_labels, dtype=T.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if T.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        preds = self.x_data[idx]\n",
    "        lbl = self.y_data[idx]\n",
    "        sample = {'predictors': preds, 'target': lbl}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def adjust_learning_rate(initial_lr, optimizer, epoch, lr_schedule=[30, 60, 90]):\n",
    "    lr = initial_lr\n",
    "    if epoch >= lr_schedule[0]:\n",
    "        lr *= 0.1\n",
    "\n",
    "    if epoch >= lr_schedule[1]:\n",
    "        lr *= 0.1\n",
    "\n",
    "    if epoch >= lr_schedule[2]:\n",
    "        lr *= 0.1\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def inference(model, dl):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    s = T.nn.Softmax(dim=-1)\n",
    "    with T.no_grad():\n",
    "        for (batch_idx, batch) in enumerate(dl):\n",
    "            X = batch['predictors']\n",
    "            Y = batch['target']\n",
    "            oupt = model(X)\n",
    "            oupt = s(oupt)\n",
    "\n",
    "            all_scores.extend(oupt.cpu().numpy())\n",
    "            all_labels.extend(Y.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    return np.array(all_scores), np.array(all_labels)\n",
    "\n",
    "\n",
    "def accuracy(model, dl):\n",
    "    all_preds, all_labels = inference(model, dl)\n",
    "    pred_indices = np.argmax(all_preds, axis=-1)\n",
    "    cumulative_acc = np.sum(pred_indices == all_labels) / len(all_labels)\n",
    "    data = {'preds':pred_indices, 'labels':all_labels}\n",
    "    with open('mf_rmf_nn_train.pkl', 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    acc_per_qubit = []\n",
    "    for i in range(5):\n",
    "        pred_qubit = pred_indices % 2\n",
    "        label_qubit = all_labels % 2\n",
    "        acc_per_qubit.append(np.sum(pred_qubit == label_qubit) / len(label_qubit))\n",
    "        pred_indices = pred_indices >> 1\n",
    "        all_labels = all_labels >> 1\n",
    "\n",
    "    return cumulative_acc, acc_per_qubit\n",
    "\n",
    "\n",
    "class Net(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hid1 = T.nn.Linear(5, 10)\n",
    "        self.hid2 = T.nn.Linear(10, 20)\n",
    "        self.oupt = T.nn.Linear(20, 32)\n",
    "\n",
    "        T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        T.nn.init.zeros_(self.hid1.bias)\n",
    "        T.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        T.nn.init.zeros_(self.hid2.bias)\n",
    "        T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        T.nn.init.zeros_(self.oupt.bias)\n",
    "        self.relu = T.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.hid1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.hid2(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.oupt(z)\n",
    "        return z\n",
    "\n",
    "class Net_rmf(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_rmf, self).__init__()\n",
    "        self.hid1 = T.nn.Linear(10, 10)\n",
    "        self.hid2 = T.nn.Linear(10, 20)\n",
    "        self.oupt = T.nn.Linear(20, 32)\n",
    "\n",
    "        T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        T.nn.init.zeros_(self.hid1.bias)\n",
    "        T.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        T.nn.init.zeros_(self.hid2.bias)\n",
    "        T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        T.nn.init.zeros_(self.oupt.bias)\n",
    "        self.relu = T.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.hid1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.hid2(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.oupt(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def train(run_pre_filter=False,\n",
    "          run_semi_sup=True,\n",
    "          run_rmf = True,\n",
    "          train_data_ratio = 1, dur=1000):\n",
    "    \n",
    "    # 0. get started\n",
    "    T.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # 1. create Dataset and DataLoader objects\n",
    "    \n",
    "    semi_sup_classifier = preclassifier()\n",
    "    rmf = relaxation_mf_classifier()\n",
    "    train_rmf_data = None\n",
    "    val_rmf_data = None\n",
    "    test_rmf_data = None\n",
    "    semi_sup_classifier.fit()\n",
    "    if run_semi_sup:\n",
    "        filtered_train_data, filtered_labels = semi_sup_classifier.predict(train_semi_sup_data)\n",
    "    fast_readout=20 - int(dur / 50)\n",
    "    boxcars = [1, 1, 9, 2, 9]\n",
    "    print('Boxcars: ', boxcars)\n",
    "    trace_classes = None\n",
    "    if run_rmf:\n",
    "        ##Get relaxtional matched filter data\n",
    "        trace_classes = semi_sup_classifier.get_traces()\n",
    "        rmf.fit(trace_classes, boxcars=boxcars)\n",
    "        #data_type => 0 for train, 1 for val, 2 for test\n",
    "        train_rmf_data = rmf.predict(data_type=0)\n",
    "        val_rmf_data = rmf.predict(data_type=1)\n",
    "        test_rmf_data = rmf.predict(data_type=2)\n",
    "            \n",
    "    best_bc = boxcars\n",
    "    #best_bc=[0, 0, 0, 0, 0]\n",
    "\n",
    "    demux_data_train, demux_data_val, demux_data_test = mf_demux_data_prep()\n",
    "    print(np.array(demux_data_train).shape)\n",
    "\n",
    "    if run_pre_filter:\n",
    "        mf_envelopes, _ = search_matched_filter_for_all_qubits(filtered_train_data, best_bc=best_bc)\n",
    "    elif run_semi_sup:\n",
    "        mf_envelopes, _ = search_matched_filter_for_all_qubits_preclass(filtered_train_data, filtered_labels, best_bc=best_bc)\n",
    "    else:\n",
    "        #mf_envelopes, _ = search_matched_filter_for_all_qubits(train_data, best_bc=best_bc)\n",
    "        mf_envelopes, _ = search_matched_filter_for_all_qubits_demux(demux_data_train, best_bc=best_bc)\n",
    "\n",
    "    no_mf = False\n",
    "\n",
    "    if no_mf and run_rmf:\n",
    "        train_ds = MFOutputDataset(train_rmf_data)\n",
    "        val_ds = MFOutputDataset(val_rmf_data)\n",
    "        test_ds = MFOutputDataset(test_rmf_data)\n",
    "    elif run_rmf:\n",
    "        train_ds =  MFOutputDataset(np.concatenate((matched_filter_preprocess_demux(demux_data_train, mf_envelopes), train_rmf_data), axis=2))\n",
    "        val_ds = MFOutputDataset(np.concatenate((matched_filter_preprocess_demux(demux_data_val, mf_envelopes), val_rmf_data), axis=2))\n",
    "        test_ds = MFOutputDataset(np.concatenate((matched_filter_preprocess_demux(demux_data_test, mf_envelopes), test_rmf_data), axis=2))\n",
    "    else:\n",
    "        train_ds = MFOutputDataset(matched_filter_preprocess_demux(demux_data_train, mf_envelopes))\n",
    "        val_ds = MFOutputDataset(matched_filter_preprocess_demux(demux_data_val, mf_envelopes))\n",
    "        test_ds = MFOutputDataset(matched_filter_preprocess_demux(demux_data_test, mf_envelopes))\n",
    "\n",
    "\n",
    "    bat_size = 512\n",
    "    train_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True)\n",
    "    val_ldr = T.utils.data.DataLoader(val_ds, batch_size=bat_size, shuffle=False)\n",
    "    test_ldr = T.utils.data.DataLoader(test_ds, batch_size=bat_size, shuffle=False)\n",
    "\n",
    "    # 2. create neural network\n",
    "    # Creating 10-50-25-32 binary NN classifier\n",
    "    if run_rmf and no_mf == False:\n",
    "        net = Net_rmf()  # .to(device)\n",
    "    else:\n",
    "        net = Net()\n",
    "\n",
    "    # 3. train network\n",
    "    net = net.train()  # set training mode\n",
    "    lrn_rate = 0.01\n",
    "    loss_obj = T.nn.CrossEntropyLoss()  # cross entropy\n",
    "    optimizer = T.optim.Adam(net.parameters(), lr=lrn_rate)\n",
    "    max_epochs = 100\n",
    "    ep_log_interval = 1\n",
    "\n",
    "\n",
    "\n",
    "    best_acc = -1\n",
    "    acc_train_itr, acc_val_itr = [], []\n",
    "    path = os.path.join(\".\", \"checkpoints/mf_nn\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with T.profiler.profile(\n",
    "        schedule=T.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=T.profiler.tensorboard_trace_handler('./logs/tb_nn'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    ") as prof:\n",
    "        for epoch in range(0, max_epochs):\n",
    "            epoch_loss = 0.0  # for one full epoch\n",
    "\n",
    "            lr = adjust_learning_rate(lrn_rate, optimizer, epoch)\n",
    "\n",
    "            for (batch_idx, batch) in enumerate(train_ldr):\n",
    "                X = batch['predictors']\n",
    "                Y = batch['target']\n",
    "                # print(X)\n",
    "                oupt = net(X)\n",
    "\n",
    "                loss_val = loss_obj(oupt, Y)\n",
    "                epoch_loss += loss_val.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "                prof.step()\n",
    "            if epoch % ep_log_interval == 0:\n",
    "                print(\"epoch = %4d   loss = %0.4f  lr = %0.4f\" % \\\n",
    "                    (epoch, epoch_loss, lr))\n",
    "\n",
    "            acc_train, acc_train_per_qubit = accuracy(net, train_ldr)\n",
    "            acc_train_itr.append(acc_train_per_qubit)\n",
    "            acc_val, acc_val_per_qubit = accuracy(net, val_ldr)\n",
    "            acc_train_itr.append(acc_val_per_qubit)\n",
    "\n",
    "            if acc_val >= best_acc:\n",
    "                best_acc = acc_val\n",
    "                T.save(net.state_dict(), os.path.join(path, 'best_epoch.pth'))\n",
    "\n",
    "    print(\"Finished  training\")\n",
    "    print(\"Best acc on val dataset: {}\".format(best_acc))\n",
    "\n",
    "    net.load_state_dict(T.load(os.path.join(path, 'best_epoch.pth')))\n",
    "    acc_test, acc_test_per_qubit = accuracy(net, test_ldr)\n",
    "    print(\"Acc on test dataset: {}\".format(acc_test))\n",
    "    print(\"Acc per qubit: {}\".format(acc_test_per_qubit))\n",
    "    print(\"Acc per qubit: {}\".format(acc_test_per_qubit))\n",
    "    return acc_test_per_qubit\n",
    "    \n",
    "\n",
    "train(run_semi_sup=False, run_pre_filter=False, run_rmf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
